{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to develop a model that predicts whether a customer will leave the bank soon. After taking a first look at the data and fixing missing and duplicate values, I will begin with feature preparation in which I will standardize all data with StandartScaler() and One-Hot Encoding. Different models will be explored and and classes will be balanced to improve model f1 score, and find the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beta Bank customers are leaving: little by little, chipping away every month. The bankers figured out it’s cheaper to save the existing customers rather than to attract new ones.\n",
    "\n",
    "We need to predict whether a customer will leave the bank soon. You have the data on clients’ past behavior and termination of contracts with the bank.\n",
    "\n",
    "Build a model with the maximum possible F1 score. To pass the project, you need an F1 score of at least 0.59. Check the F1 for the test set.\n",
    "\n",
    "Additionally, measure the AUC-ROC metric and compare it with the F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading necessary libraries.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import  accuracy_score, precision_recall_curve, roc_auc_score, confusion_matrix, f1_score, make_scorer, recall_score, precision_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Data.\n",
    "data = pd.read_csv('/datasets/Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at 'data'.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Exited'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Exited'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Geography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CreditScore'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datatypes seem to be correct, except Tenure which seems like it could be 'int64'. Column names are fine as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing values.\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only column with missing values is 'Tenure', which will be treated as having 0 years of Tenure. This is becuase the other features are still valuable in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Duplicate Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chekcing for duplicate values.\n",
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicated rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping 'Surname', 'CustomerId', and 'RowNumber' from 'data'.\n",
    "data.drop(columns=['Surname', 'CustomerId', 'RowNumber'], inplace=True)\n",
    "data.drop(columns=['CustomerId'], inplace=True)\n",
    "data.drop(columns=['RowNumber'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at unique values in 'Geography'.\n",
    "data['Geography'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling missing values in 'Tenure' with 0.\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns have been dropped as the data like 'RowNumber', 'Surname', and 'CustomerId' give no insight to wether a customer may leave the bank. Rows with missing 'Tenure' values are filled with 0 as the rest of the data is valuable for it's insights into customer behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning to 'data_ohe' the dataframe encoded using One-Hot Encoding.\n",
    "data_ohe = pd.get_dummies(data, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing 'data_ohe'.\n",
    "data_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning to 'target' the 'Exited' column of 'data_ohe'.\n",
    "target = data_ohe['Exited']\n",
    "\n",
    "# Assigning to 'features' all other columns but 'Exited' from 'data_ohe'.\n",
    "features = data_ohe.drop('Exited', axis=1)\n",
    "\n",
    "# Splitting data into train, validation, and test sets\n",
    "features_train, features_temp, target_train, target_temp = train_test_split(\n",
    "    features, target, test_size=0.4, random_state=12345\n",
    ")\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(\n",
    "    features_temp, target_temp, test_size=0.5, random_state=12345\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of numerical features in the dataset.\n",
    "numeric = ['Age', 'CreditScore', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary','Geography_Germany','Geography_Spain', 'Gender_Male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing StandardScaler to standardize numerical features.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fiting the scaler to the training set's numerical features.\n",
    "scaler.fit(features_train[numeric])\n",
    "\n",
    "#Transforming the numerical features of the training set using the fitted scaler.\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "\n",
    "#Transforming the numerical features of the validation set using the same scaler.\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "\n",
    "#Transforming the numerical features of the test set using the same scaler.\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])\n",
    "\n",
    "#Printing the first few rows of the transformed training set to inspect the changes.\n",
    "print(features_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all features should be considered equally important before the algorithm's execution, I have standardized the data using StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a custom scoring function using F1 score.\n",
    "scorer = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameters grid.\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "#Initializing DecisionTreeClassifier model.\n",
    "dtc_model = DecisionTreeClassifier(random_state=12345)\n",
    "\n",
    "#Searching for the best combination of hyperparameters.\n",
    "grid_search = GridSearchCV(dtc_model, param_grid, cv=5, scoring=scorer)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'best_dtc_params' the best hyperparameters.\n",
    "best_dtc_params = grid_search.best_params_\n",
    "\n",
    "#Training Model using 'features_train' and 'target train'.\n",
    "best_dtc_model = DecisionTreeClassifier(**best_dtc_params, random_state=12345)\n",
    "best_dtc_model.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on training values.\n",
    "train_accuracy = best_dtc_model.score(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on validation values.\n",
    "valid_accuracy = best_dtc_model.score(features_valid, target_valid)\n",
    "\n",
    "#Assigning to 'predicted_valid' model predictions using 'features_valid'.\n",
    "predicted_valid = best_dtc_model.predict(features_valid)\n",
    "\n",
    "#Printing the best hyperparameters found by grid search.\n",
    "print(\"Best hyperparameters:\", best_dtc_params)\n",
    "\n",
    "#Printing the accuracy on the training set.\n",
    "print(\"Accuracy on the training set:\", train_accuracy)\n",
    "\n",
    "#Printing the accuracy on the validation set.\n",
    "print(\"Accuracy on the validation set:\", valid_accuracy)\n",
    "\n",
    "#Printing the F1 score for the validation set predictions.\n",
    "print('F1 Score:', f1_score(target_valid, predicted_valid))\n",
    "\n",
    "#Printing the recall score for the validation set predictions.\n",
    "print('Recall Score:', recall_score(target_valid, predicted_valid))\n",
    "\n",
    "#Printing the precision score for the validation set predictions.\n",
    "print('Precision Score:', precision_score(target_valid, predicted_valid))\n",
    "\n",
    "#Calculating the probabilities of class 1 for the validation set.\n",
    "probabilities_valid = best_dtc_model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "#Computing the false positive rate, true positive rate, and thresholds for the ROC curve.\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "\n",
    "#Compingting the area under the ROC curve (AUC-ROC).\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "print('AUC-ROC:', auc_roc)\n",
    "\n",
    "#Ploting the ROC curve.\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree Classifier model shows decent performance, with relatively high accuracy and AUC-ROC. However, there is room for improvement, especially in terms of recall and F1 score, indicating that the model could better identify positive instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameters grid.\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "#Initializing LogisticRegression model.\n",
    "lr_model = LogisticRegression(random_state=12345)\n",
    "\n",
    "#Searching for the best combination of hyperparameters.\n",
    "grid_search = GridSearchCV(lr_model, param_grid, cv=5, scoring=scorer)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'best_params' the best hyperparameters.\n",
    "best_lr_params = grid_search.best_params_\n",
    "\n",
    "#Training Model using 'features_train' and 'target train'.\n",
    "best_lr_model = LogisticRegression(**best_lr_params, random_state=12345)\n",
    "best_lr_model.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on training values.\n",
    "train_accuracy = best_lr_model.score(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on validation values.\n",
    "valid_accuracy = best_lr_model.score(features_valid, target_valid)\n",
    "\n",
    "#Assigning to 'predicted_valid' model predictions using 'features_valid'.\n",
    "predicted_valid = best_lr_model.predict(features_valid)\n",
    "\n",
    "#Printing findings.\n",
    "print(\"Best hyperparameters:\", best_lr_params)\n",
    "print(\"Accuracy on the training set:\", train_accuracy)\n",
    "print(\"Accuracy on the validation set:\", valid_accuracy)\n",
    "print('F1 Score:', f1_score(target_valid, predicted_valid))\n",
    "print('Recall Score:', recall_score(target_valid, predicted_valid))\n",
    "print('Precision Score:', precision_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = best_lr_model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model demonstrates moderate performance, with accuracy and AUC-ROC values indicating some ability to discriminate between classes. However, the F1 score and recall are relatively low, suggesting that the model struggles to identify positive instances effectively. There may be room for improvement through further tuning or exploration of different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameters grid.\n",
    "param_grid = {\n",
    "    'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "#Initializing RandomForestClassifier model.\n",
    "rfc_model = RandomForestClassifier(random_state=12345)\n",
    "\n",
    "#Searching for the best combination of hyperparameters.\n",
    "grid_search = GridSearchCV(rfc_model, param_grid, cv=7, scoring=scorer)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'best_params' the best hyperparameters.\n",
    "best_rfc_params = grid_search.best_params_\n",
    "\n",
    "#Training Model using 'features_train' and 'target train'.\n",
    "best_rfc_model = RandomForestClassifier(**best_rfc_params, random_state=12345)\n",
    "best_rfc_model.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on training values.\n",
    "train_accuracy = best_rfc_model.score(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on validation values.\n",
    "valid_accuracy = best_rfc_model.score(features_valid, target_valid)\n",
    "\n",
    "#Assigning to 'predicted_valid' model predictions using 'features_valid'.\n",
    "predicted_valid = best_rfc_model.predict(features_valid)\n",
    "\n",
    "#Printing findings.\n",
    "print(\"Best hyperparameters:\", best_rfc_params)\n",
    "print(\"Accuracy on the training set:\", train_accuracy)\n",
    "print(\"Accuracy on the validation set:\", valid_accuracy)\n",
    "print('F1 Score:', f1_score(target_valid, predicted_valid))\n",
    "print('Recall Score:', recall_score(target_valid, predicted_valid))\n",
    "print('Precision Score:', precision_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = best_rfc_model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Classifier model demonstrates strong performance, with high accuracy and AUC-ROC values indicating good discrimination between classes. The F1 score and recall are relatively balanced, suggesting effective identification of positive instances. This model appears to be well-tuned and performs well on both training and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Initializing DecisionTreeClassifier model.\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m dtc_model \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Searching for the best combination of hyperparameters.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(dtc_model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mscorer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "#Defining the hyperparameters grid.\n",
    "param_grid = {\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "#Initializing DecisionTreeClassifier model.\n",
    "dtc_model = DecisionTreeClassifier(random_state=12345)\n",
    "\n",
    "#Searching for the best combination of hyperparameters.\n",
    "grid_search = GridSearchCV(dtc_model, param_grid, cv=5, scoring=scorer)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'best_dtc_params' the best hyperparameters.\n",
    "best_dtc_params = grid_search.best_params_\n",
    "\n",
    "#Training Model using 'features_train' and 'target train'.\n",
    "best_dtc_model = DecisionTreeClassifier(**best_dtc_params, random_state=12345)\n",
    "best_dtc_model.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on training values.\n",
    "train_accuracy = best_dtc_model.score(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on validation values.\n",
    "valid_accuracy = best_dtc_model.score(features_valid, target_valid)\n",
    "\n",
    "#Assigning to 'predicted_valid' model predictions using 'features_valid'.\n",
    "predicted_valid = best_dtc_model.predict(features_valid)\n",
    "\n",
    "#Printing findings.\n",
    "print(\"Best hyperparameters:\", best_dtc_params)\n",
    "print(\"Accuracy on the training set:\", train_accuracy)\n",
    "print(\"Accuracy on the validation set:\", valid_accuracy)\n",
    "print('F1 Score:', f1_score(target_valid, predicted_valid))\n",
    "print('Recall Score:', recall_score(target_valid, predicted_valid))\n",
    "print('Precision Score:', precision_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = best_dtc_model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DecisionTreeClassifier model using 'balanced' for class_weight demonstrates reasonable performance, with a balanced F1 score and relatively high recall, suggesting effective identification of positive instances. However, the precision score is comparatively lower, indicating a higher rate of false positive predictions. The AUC-ROC value suggests good discrimination between classes. This model may benefit from further optimization or exploration of different algorithms to improve precision while maintaining high recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameters grid.\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "#Initializing LogisticRegression model.\n",
    "lr_model = LogisticRegression(random_state=12345)\n",
    "\n",
    "#Searching for the best combination of hyperparameters.\n",
    "grid_search = GridSearchCV(lr_model, param_grid, cv=5, scoring=scorer)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'best_params' the best hyperparameters.\n",
    "best_lr_params = grid_search.best_params_\n",
    "\n",
    "#Training Model using 'features_train' and 'target train'.\n",
    "best_lr_model = LogisticRegression(**best_lr_params, random_state=12345)\n",
    "best_lr_model.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on training values.\n",
    "train_accuracy = best_lr_model.score(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on validation values.\n",
    "valid_accuracy = best_lr_model.score(features_valid, target_valid)\n",
    "\n",
    "#Assigning to 'predicted_valid' model predictions using 'features_valid'.\n",
    "predicted_valid = best_lr_model.predict(features_valid)\n",
    "\n",
    "#Printing findings.\n",
    "print(\"Best hyperparameters:\", best_lr_params)\n",
    "print(\"Accuracy on the training set:\", train_accuracy)\n",
    "print(\"Accuracy on the validation set:\", valid_accuracy)\n",
    "print('F1 Score:', f1_score(target_valid, predicted_valid))\n",
    "print('Recall Score:', recall_score(target_valid, predicted_valid))\n",
    "print('Precision Score:', precision_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = best_lr_model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LogisticRegression model using 'balanced' class weight demonstrates moderate performance, with relatively low accuracy and precision scores. The recall score is higher, indicating a better ability to identify positive instances, but the precision is comparatively lower, suggesting a higher rate of false positive predictions. The AUC-ROC value suggests moderate discrimination between classes. This model may benefit from further optimization or exploration of different algorithms to improve overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameters grid.\n",
    "param_grid = {\n",
    "    'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "#Initializing RandomForestClassifier model.\n",
    "rfc_model = RandomForestClassifier(random_state=12345)\n",
    "\n",
    "#Searching for the best combination of hyperparameters.\n",
    "grid_search = GridSearchCV(rfc_model, param_grid, cv=7, scoring=scorer)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'best_params' the best hyperparameters.\n",
    "best_rfc_params = grid_search.best_params_\n",
    "\n",
    "#Training Model using 'features_train' and 'target train'.\n",
    "best_rfc_model = RandomForestClassifier(**best_rfc_params, random_state=12345)\n",
    "best_rfc_model.fit(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on training values.\n",
    "train_accuracy = best_rfc_model.score(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on validation values.\n",
    "valid_accuracy = best_rfc_model.score(features_valid, target_valid)\n",
    "\n",
    "#Assigning to 'predicted_valid' model predictions using 'features_valid'.\n",
    "predicted_valid = best_rfc_model.predict(features_valid)\n",
    "\n",
    "#Printing findings.\n",
    "print(\"Best hyperparameters:\", best_rfc_params)\n",
    "print(\"Accuracy on the training set:\", train_accuracy)\n",
    "print(\"Accuracy on the validation set:\", valid_accuracy)\n",
    "print('F1 Score:', f1_score(target_valid, predicted_valid))\n",
    "print('Recall Score:', recall_score(target_valid, predicted_valid))\n",
    "print('Precision Score:', precision_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = best_rfc_model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomForestClassifier model using 'class_weight': 'balanced' demonstrates strong performance, with high accuracy and AUC-ROC values indicating good discrimination between classes. The F1 score and recall are relatively balanced, suggesting effective identification of positive instances. This model appears to be well-tuned and performs well on both training and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating upsampling function\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345\n",
    "    )\n",
    "\n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing variables to store the best F1 score and corresponding repeat value.\n",
    "best_f1 = 0\n",
    "best_repeat = 0\n",
    "\n",
    "#Iterating over a range of values (0 to 19) for the repeat parameter.\n",
    "for i in range(20):\n",
    "    \n",
    "    #Upsampling the training data using the current value of the repeat parameter.\n",
    "    features_upsampled, target_upsampled = upsample(features_train, target_train, i)\n",
    "\n",
    "    #Initializing a RandomForestClassifier model with the best hyperparameters.\n",
    "    best_rfc_model = RandomForestClassifier(**best_rfc_params, random_state=12345)\n",
    "    \n",
    "    #Training the RandomForestClassifier model on the upsampled data.\n",
    "    best_rfc_model.fit(features_upsampled, target_upsampled)\n",
    "\n",
    "    #Predicting the target variable for the validation set using the trained model.\n",
    "    predicted_valid = best_rfc_model.predict(features_valid)\n",
    "    \n",
    "    #Calculating the F1 score for the predictions on the validation set.\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "    \n",
    "    #Updating the best F1 score and corresponding repeat value if the current F1 score is higher.\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_repeat = i\n",
    "\n",
    "#Printing the best repeat value and corresponding F1 score\n",
    "print('Best repeat n:', best_repeat)\n",
    "print('Best F1:', best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    #Separating features and target for each class (0 and 1).\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    #Downsampling the majority class (class 0) to match the specified fraction.\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)]  # Sample fraction of class 0.\n",
    "        + [features_ones]  #Combining with all instances of class 1\n",
    "    )\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)]  # Sample fraction of class 0.\n",
    "        + [target_ones]  #Combining with all instances of class 1.\n",
    "    )\n",
    "\n",
    "    #Shuffling the downsampled data to randomize the order.\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345\n",
    "    )\n",
    "\n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampling the majority class in the training set to achieve a balanced dataset.\n",
    "features_downsampled, target_downsampled = downsample(features_train, target_train, 1)\n",
    "\n",
    "#Initializing a RandomForestClassifier model with the best hyperparameters.\n",
    "best_rfc_model = RandomForestClassifier(**best_rfc_params, random_state=12345)\n",
    "\n",
    "#Training the RandomForestClassifier model on the downsampled training data.\n",
    "best_rfc_model.fit(features_downsampled, target_downsampled)\n",
    "\n",
    "#Predicting the target variable for the validation set using the trained model.\n",
    "predicted_valid = best_rfc_model.predict(features_valid)\n",
    "\n",
    "#Calculating the F1 score for the predictions on the validation set.\n",
    "print('F1:', f1_score(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upsampling the minority class in the training set to achieve a balanced dataset.\n",
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 8)\n",
    "\n",
    "#Training Model using 'features_upsampled' and 'target_upsampled train'.\n",
    "best_rfc_model = RandomForestClassifier(**best_rfc_params, random_state=12345)\n",
    "best_rfc_model.fit(features_upsampled, target_upsampled)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on training values.\n",
    "train_accuracy = best_rfc_model.score(features_train, target_train)\n",
    "\n",
    "#Assigning to 'train_accuracy' score based on validation values.\n",
    "valid_accuracy = best_rfc_model.score(features_valid, target_valid)\n",
    "\n",
    "#Assigning to 'predicted_valid' model predictions using 'features_valid'.\n",
    "predicted_valid = best_rfc_model.predict(features_valid)\n",
    "\n",
    "#Printing findings.\n",
    "print(\"Best hyperparameters:\", best_rfc_params)\n",
    "print(\"Accuracy on the training set:\", train_accuracy)\n",
    "print(\"Accuracy on the validation set:\", valid_accuracy)\n",
    "print('F1 Score:', f1_score(target_valid, predicted_valid))\n",
    "print('Recall Score:', recall_score(target_valid, predicted_valid))\n",
    "print('Precision Score:', precision_score(target_valid, predicted_valid))\n",
    "\n",
    "probabilities_valid = best_rfc_model.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)\n",
    "auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning to 'test_accuracy' score based on test values.\n",
    "test_accuracy = best_rfc_model.score(features_test, target_test)\n",
    "\n",
    "#Assigning to 'predicted_test' model predictions using 'features_test'.\n",
    "predicted_test = best_rfc_model.predict(features_test)\n",
    "\n",
    "#Printing findings.\n",
    "print(\"Accuracy on the training set:\", train_accuracy)\n",
    "print(\"Accuracy on the test set:\", test_accuracy)\n",
    "print('F1 Score:', f1_score(target_test, predicted_test))\n",
    "print('Recall Score:', recall_score(target_test, predicted_test))\n",
    "print('Precision Score:', precision_score(target_test, predicted_test))\n",
    "\n",
    "probabilities_test = best_rfc_model.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_valid[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_test)\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final and most effective model was the RandomForestClassifier using balanced as the class weight, a max depth of 10 and 13 estimators using upsampled data, and produced a balanced F1 score and relatively high recall, indicating effective identification of positive instances. The precision score is moderate, suggesting a reasonable proportion of true positive predictions among all positive predictions made by the model. The AUC-ROC value indicates good discrimination between classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
